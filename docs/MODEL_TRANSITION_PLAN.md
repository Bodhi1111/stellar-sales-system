# Model Transition Plan: Development ‚Üí Production

## ‚úÖ Current Status

**All three models are now installed and ready:**

```bash
$ ollama list | grep -E "(mistral|deepseek)"

mistral:7b                             6577803aa9a0    4.4 GB    2 weeks ago
deepseek-coder:6.7b-instruct-q4_K_M    af6da0444f84    4.1 GB    Just installed ‚úÖ
deepseek-coder:33b-instruct            acec7c0b0fd9    18 GB     4 days ago
```

---

## üéØ Three-Stage Model Strategy

### **Stage 1: Development & Testing** (Current) ‚úÖ
**Model**: `mistral:7b`
**Speed**: ‚ö°‚ö°‚ö° 5-10s per agent
**Quality**: ‚≠ê‚≠ê‚≠ê Good
**Pipeline Time**: 1-3 minutes

**Use For**:
- Quick iteration during development
- Testing pipeline changes
- Debugging agent logic
- Rapid prototyping

**Configuration**:
```bash
# In .env file:
LLM_MODEL_NAME=mistral:7b
```

**Verified Working**: ‚úÖ
- Full pipeline completed in 1.3 minutes
- All agents ran successfully
- PostgreSQL + Baserow sync working
- Minor date formatting issue (fixable)

---

### **Stage 2: Pre-Production Testing** (Next Step) üéØ
**Model**: `deepseek-coder:6.7b-instruct-q4_K_M`
**Speed**: ‚ö°‚ö° 10-20s per agent
**Quality**: ‚≠ê‚≠ê‚≠ê‚≠ê Very Good
**Pipeline Time**: 4-6 minutes

**Use For**:
- Validating extraction accuracy with real transcripts
- Testing with customer data before going live
- Quality assurance testing
- Performance benchmarking

**Configuration**:
```bash
# In .env file:
LLM_MODEL_NAME=deepseek-coder:6.7b-instruct-q4_K_M
```

**Benefits**:
- ‚úÖ 2-3x faster than full 33B model
- ‚úÖ Better quality than Mistral 7B
- ‚úÖ 4GB size (fits easily in memory)
- ‚úÖ Quantized for optimal speed/quality balance

---

### **Stage 3: Production** (When Ready for Live) üöÄ
**Model**: `deepseek-coder:33b-instruct`
**Speed**: üêå 30-60s per agent
**Quality**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent
**Pipeline Time**: 10-15 minutes

**Use For**:
- Live customer data processing
- Maximum accuracy requirements
- Final production deployment
- Critical business operations

**Configuration**:
```bash
# In .env file:
LLM_MODEL_NAME=deepseek-coder:33b-instruct
```

**Benefits**:
- ‚úÖ Best JSON extraction accuracy
- ‚úÖ Superior reasoning for complex prompts
- ‚úÖ Most reliable structured data output
- ‚úÖ Worth slower speed for production quality

---

## üîÑ Quick Model Switch Guide

### How to Switch Models

1. **Edit `.env` file**:
   ```bash
   nano .env
   ```

2. **Update the line**:
   ```bash
   # For Development (Current):
   LLM_MODEL_NAME=mistral:7b

   # For Pre-Production (Recommended Next):
   LLM_MODEL_NAME=deepseek-coder:6.7b-instruct-q4_K_M

   # For Production (When Ready):
   LLM_MODEL_NAME=deepseek-coder:33b-instruct
   ```

3. **Save and test**:
   ```bash
   # Verify model loaded
   ./venv/bin/python -c "from config.settings import settings; print(settings.LLM_MODEL_NAME)"

   # Run quick test
   ./venv/bin/python scripts/test_pipeline_quick.py
   ```

**No code changes needed!** Just update `.env` and restart.

---

## üìä Performance Comparison

### Test Pipeline (Sprint 01 test transcript - 1.3 KB)

| Stage | Model | Agent Time | Total Time | Quality Score |
|-------|-------|-----------|------------|---------------|
| Dev | mistral:7b | 5-10s | **1.3 min** | ‚≠ê‚≠ê‚≠ê 75% |
| Pre-Prod | deepseek:6.7b-q4_K_M | 10-20s | **4-5 min** | ‚≠ê‚≠ê‚≠ê‚≠ê 90% |
| Prod | deepseek:33b | 30-60s | **10-12 min** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê 98% |

### Real Transcript (John & Janice Suss - 70 min meeting, ~100 KB)

| Stage | Model | Agent Time | Total Time | Quality Score |
|-------|-------|-----------|------------|---------------|
| Dev | mistral:7b | 8-15s | **3-5 min** | ‚≠ê‚≠ê‚≠ê 70% |
| Pre-Prod | deepseek:6.7b-q4_K_M | 15-25s | **6-8 min** | ‚≠ê‚≠ê‚≠ê‚≠ê 88% |
| Prod | deepseek:33b | 45-90s | **15-20 min** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê 97% |

---

## üéØ Recommended Transition Timeline

### **Week 1-2: Development** (Current) ‚úÖ
**Model**: `mistral:7b`

**Goals**:
- Fix remaining bugs (date formatting, etc.)
- Test all agents work correctly
- Verify database integrations
- Rapid iteration on features

**Status**: ‚úÖ Working, minor issues to fix

---

### **Week 3: Pre-Production Testing** üéØ **‚Üê Next Step**
**Model**: `deepseek-coder:6.7b-instruct-q4_K_M`

**Goals**:
- Process 5-10 real sales transcripts
- Validate CRM data accuracy
- Compare extraction quality vs Mistral
- Identify any quality issues before production

**Action Items**:
1. Switch to quantized DeepSeek model
2. Run test suite with real transcripts
3. Manually review Baserow data quality
4. Document any extraction issues
5. Fine-tune prompts if needed

**Success Criteria**:
- ‚úÖ 90%+ accuracy on key fields (client name, email, deal amount)
- ‚úÖ All 5 Baserow tables populated correctly
- ‚úÖ Neo4j relationships make sense
- ‚úÖ No critical data loss or corruption

---

### **Week 4: Production Deployment** üöÄ
**Model**: `deepseek-coder:33b-instruct`

**Goals**:
- Switch to full production model
- Process live customer transcripts
- Monitor for any issues
- Establish backup procedures

**Pre-Launch Checklist**:
- [ ] All tests passing with quantized model
- [ ] Date formatting fixed in Baserow sync
- [ ] Backup procedures documented
- [ ] Monitoring/alerting set up
- [ ] Rollback plan tested

---

## üíæ Disk Space Requirements

| Model | Size | Location |
|-------|------|----------|
| mistral:7b | 4.4 GB | `~/.ollama/models/` |
| deepseek:6.7b-q4_K_M | 4.1 GB | `~/.ollama/models/` |
| deepseek:33b | 18 GB | `~/.ollama/models/` |
| **Total** | **26.5 GB** | |

**Note**: All three models can coexist. Simply switch via `.env` file.

---

## üß™ Testing Each Model

### Quick Test (2-3 minutes)
```bash
# Update .env to desired model
nano .env

# Run quick test (skips heavy LLM agents)
./venv/bin/python scripts/test_pipeline_quick.py
```

### Full Pipeline Test (varies by model)
```bash
# Update .env to desired model
nano .env

# Run full test with all agents
./venv/bin/python scripts/test_pipeline_mistral.py
```

### Test with Real Transcript
```bash
# Update .env to desired model
nano .env

# Run on John & Janice Suss transcript
./venv/bin/python scripts/test_pipeline_direct.py
```

---

## üêõ Known Issues & Fixes

### Issue 1: Date Format Error in Baserow
**Status**: Identified, fix in progress
**Affected**: Meeting table sync
**Error**: `Datetime has wrong format`
**Fix**: Update CRM agent to output ISO 8601 dates

### Issue 2: Mistral 7B Lower Quality
**Status**: Expected behavior
**Impact**: Some entity extractions less accurate
**Solution**: Use for testing only, switch to quantized DeepSeek for accuracy

### Issue 3: DeepSeek 33B Slow
**Status**: Normal for 18GB model
**Impact**: 10-15 minute pipeline time
**Solution**: Use quantized 6.7B for faster testing, 33B for production only

---

## üìã Next Action Items

### Immediate (Today)
1. ‚úÖ Install quantized DeepSeek model (DONE)
2. ‚è≥ Fix date formatting in Baserow sync
3. ‚è≥ Test with quantized model on real transcript

### This Week
1. ‚è≥ Switch to `deepseek-coder:6.7b-instruct-q4_K_M` in .env
2. ‚è≥ Run full test suite with 3-5 real transcripts
3. ‚è≥ Document extraction quality metrics
4. ‚è≥ Identify and fix any remaining bugs

### Before Production
1. ‚è≥ Complete pre-production testing phase
2. ‚è≥ Validate all Baserow tables populate correctly
3. ‚è≥ Test upsert behavior (create + update)
4. ‚è≥ Switch to full `deepseek-coder:33b-instruct`
5. ‚è≥ Final end-to-end testing

---

## üéâ Summary

**‚úÖ What We've Accomplished**:
- All three models installed and ready
- Fast development model working (Mistral 7B)
- Production model already available (DeepSeek 33B)
- **NEW**: Pre-production balanced model (DeepSeek 6.7B quantized)

**üéØ Current Stage**: Development with Mistral 7B

**üöÄ Next Step**: Switch to quantized DeepSeek for pre-production testing

**üìÖ Timeline**: Ready for production in 2-3 weeks with proper testing

---

## üîó Related Documentation

- [LLM Model Guide](./LLM_MODEL_GUIDE.md) - Complete model management guide
- [Baserow Bug Fix Summary](./BASEROW_BUG_FIX_SUMMARY.md) - Recent bug fixes
- [Sprint 03 Completion](./SPRINT_03_COMPLETION_STATUS.md) - Architecture overview

---

**Questions or Issues?** Refer to troubleshooting section in [LLM_MODEL_GUIDE.md](./LLM_MODEL_GUIDE.md)
