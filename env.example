# Stellar Sales System - Environment Configuration Template
# Copy this file to .env and fill in your actual values

# ==============================================
# DATABASE CONFIGURATION
# ==============================================

# PostgreSQL (Primary data storage)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=stellar_sales
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_secure_password_here

# ==============================================
# VECTOR DATABASE
# ==============================================

# Qdrant (Semantic search and embeddings)
QDRANT_URL=http://localhost:6333

# ==============================================
# LLM CONFIGURATION
# ==============================================

# Ollama (Local LLM inference)
# Make sure Ollama is running: ollama serve
OLLAMA_API_URL=http://localhost:11434/api/generate

# LLM Model Selection
# Options: deepseek-coder:33b-instruct (recommended), mistral:7b, llama3.1:8b
LLM_MODEL_NAME=deepseek-coder:33b-instruct

# Embedding Model (for semantic search)
# Recommended: BAAI/bge-base-en-v1.5 (768 dimensions)
EMBEDDING_MODEL_NAME=BAAI/bge-base-en-v1.5

# ==============================================
# CRM INTEGRATION (BASEROW)
# ==============================================

# Baserow (Open-source Airtable alternative)
BASEROW_URL=http://localhost:8080

# Get your token from: Baserow Settings > Account > API Tokens
BASEROW_TOKEN=your_baserow_api_token_here

# Database and Table IDs (found in Baserow URL)
# Example URL: http://localhost:8080/database/174/table/704
BASEROW_DATABASE_ID=174

# CRM Tables (6 tables)
BASEROW_CLIENTS_ID=704
BASEROW_MEETINGS_ID=705
BASEROW_DEALS_ID=706
BASEROW_COMMUNICATIONS_ID=707
BASEROW_SALES_COACHING_ID=708
BASEROW_CHUNKS_ID=709

# ==============================================
# OBSERVABILITY & MONITORING
# ==============================================

# LangFuse (Local Self-Hosted Observability - RECOMMENDED)
# Complete pipeline tracing and visualization
# Setup Steps:
#   1. Start LangFuse: docker-compose -f docker-compose.langfuse.yml up -d
#   2. Open UI: http://localhost:3000
#   3. Create account and login
#   4. Go to Settings → API Keys → Create new API key
#   5. Copy the public key (starts with pk-lf-) and secret key (starts with sk-lf-) below
LANGFUSE_PUBLIC_KEY=pk-lf-your-public-key-here
LANGFUSE_SECRET_KEY=sk-lf-your-secret-key-here
LANGFUSE_HOST=http://localhost:3000

# LangSmith (LangChain Cloud Observability - ALTERNATIVE)
# Sign up at: https://smith.langchain.com/
# Get API key from: Settings > API Keys
# Note: LangFuse is recommended for local development
LANGCHAIN_TRACING_V2=false
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
LANGCHAIN_API_KEY=your_langsmith_api_key_here
LANGCHAIN_PROJECT=stellar-sales-system

# ==============================================
# OPTIONAL INTEGRATIONS
# ==============================================

# N8N (Workflow automation - DEPRECATED, use LangSmith instead)
# Only needed if using N8N RAG workflow
N8N_LICENSE_KEY=

# ==============================================
# SETUP INSTRUCTIONS
# ==============================================

# 1. Copy this file to .env:
#    cp env.example .env

# 2. Start Docker services:
#    docker-compose up -d

# 3. Pull LLM model:
#    ollama pull deepseek-coder:33b-instruct

# 4. Install spaCy model:
#    python -m spacy download en_core_web_sm

# 5. Initialize database:
#    python scripts/init_db.py

# 6. Run the pipeline:
#    python orchestrator/pipeline.py

# ==============================================
# NOTES
# ==============================================

# - Baserow table IDs can be found in your Baserow instance
# - LLM model affects performance: 33B (slower, accurate) vs 7B (faster)
# - Qdrant and PostgreSQL data is persisted in Docker volumes
# - For production, use stronger passwords and rotate credentials regularly

