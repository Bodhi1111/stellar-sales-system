# LLM Model Quick Reference Card

## ğŸš€ Quick Model Switch

Edit `.env` and change one line:

```bash
# Current: Development (Fast)
LLM_MODEL_NAME=mistral:7b

# Next: Pre-Production (Balanced) â† RECOMMENDED NEXT STEP
LLM_MODEL_NAME=deepseek-coder:6.7b-instruct-q4_K_M

# Later: Production (Best Quality)
LLM_MODEL_NAME=deepseek-coder:33b-instruct
```

Then restart your test!

---

## ğŸ“Š Model Comparison (At a Glance)

| Metric | Mistral 7B | DeepSeek 6.7B (Q) | DeepSeek 33B |
|--------|------------|-------------------|--------------|
| **Size** | 4.4 GB | 4.1 GB | 18 GB |
| **Speed** | âš¡âš¡âš¡ Fast | âš¡âš¡ Medium | ğŸŒ Slow |
| **Quality** | â­â­â­ Good | â­â­â­â­ Very Good | â­â­â­â­â­ Excellent |
| **Pipeline Time** | 1-3 min | 4-6 min | 10-15 min |
| **Use Case** | Development | Pre-Production | Production |
| **Status** | âœ… Current | âœ… Installed | âœ… Ready |

---

## âœ… All Models Installed

```
mistral:7b                             4.4 GB  âœ…
deepseek-coder:6.7b-instruct-q4_K_M    4.1 GB  âœ…
deepseek-coder:33b-instruct            18 GB   âœ…
```

**Total disk space**: 26.5 GB

---

## ğŸ¯ Recommended Path

1. **Now**: Development with `mistral:7b` âœ…
2. **Next Week**: Testing with `deepseek-coder:6.7b-instruct-q4_K_M` ğŸ¯
3. **Production**: Deploy with `deepseek-coder:33b-instruct` ğŸš€

---

## ğŸ“ Quick Test Commands

```bash
# Verify current model
./venv/bin/python -c "from config.settings import settings; print(settings.LLM_MODEL_NAME)"

# Quick test (2-3 minutes)
./venv/bin/python scripts/test_pipeline_quick.py

# Full pipeline test
./venv/bin/python scripts/test_pipeline_mistral.py

# Check installed models
ollama list
```

---

**Full docs**: [MODEL_TRANSITION_PLAN.md](docs/MODEL_TRANSITION_PLAN.md)
