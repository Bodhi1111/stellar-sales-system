# Playbook: SPRINT 03 - Upgrading Specialist Agents & Finalizing Workflow

## Sprint Goal

To refactor our key specialist agents to function as "tools" within the new reasoning engine and to resolve the final architectural challenges in our data pipeline. By the end of this sprint, the `SalesCopilotAgent` will be a powerful, multi-modal retrieval tool, and our data persistence logic will be robust and correctly ordered.

---

### Epic 3.1: Solving the Persistence Dependency

**Objective:** To resolve the critical dependency identified in Sprint 1, where the `EmbedderAgent` needs a `transcript_id` from the database *before* the final data is ready to be saved. We will solve this by splitting our persistence logic into two distinct steps.

#### **Task 3.1.1: Refactor the `PersistenceAgent` into Two New Agents**

* **Rationale:** The Single Responsibility Principle guides us here. One agent cannot both create the initial record at the beginning of the workflow and update it at the end. We will create two new, highly-focused agents: one for initialization and one for finalization.
* **Step 1: Delete the Old Agent**
    * Delete the file `agents/persistence/persistence_agent.py`.
* **Step 2: Create the `InitialPersistenceAgent`**
    * Create a new file: `agents/persistence/initial_persistence_agent.py`.
    * The following code goes into this new file. Its only job is to create a placeholder record in PostgreSQL and return the new `id`.

    ```python
    from pathlib import Path
    from typing import Dict, Any

    from agents.base_agent import BaseAgent
    from config.settings import Settings
    from core.database.postgres import db_manager
    from core.database.models import Transcript

    class InitialPersistenceAgent(BaseAgent):
        """
        Creates an initial, placeholder transcript record in PostgreSQL.
        Its sole purpose is to generate a primary key (transcript_id) that
        can be used by other agents to link their data.
        """
        async def run(self, file_path: Path) -> Dict[str, Any]:
            print(f"ðŸ’¾ InitialPersistenceAgent: Creating placeholder for {file_path.name}...")
            transcript_id = None
            try:
                await db_manager.initialize()
                async with db_manager.session_context() as session:
                    # Create a minimal record just to get an ID
                    new_transcript = Transcript(
                        filename=file_path.name,
                    )
                    session.add(new_transcript)
                    await session.commit()
                    transcript_id = new_transcript.id
                print(f"   - Generated Transcript ID: {transcript_id}")
                return {"transcript_id": transcript_id}
            except Exception as e:
                print(f"   âŒ ERROR in InitialPersistenceAgent: {e}")
                return {"transcript_id": None} # Return None on failure
    ```

* **Step 3: Create the `FinalPersistenceAgent`**
    * Create a new file: `agents/persistence/final_persistence_agent.py`.
    * The following code goes into this new file. Its job is to find the existing record and update it with all the final data.

    ```python
    from typing import Dict, Any
    from sqlalchemy.future import select

    from agents.base_agent import BaseAgent
    from config.settings import Settings
    from core.database.postgres import db_manager
    from core.database.models import Transcript

    class FinalPersistenceAgent(BaseAgent):
        """
        Updates the final, comprehensive record in PostgreSQL using the
        transcript_id generated by the InitialPersistenceAgent.
        """
        async def run(self, state: Dict[str, Any]) -> Dict[str, Any]:
            transcript_id = state.get("transcript_id")
            if not transcript_id:
                return {"final_persistence_status": "error", "message": "Missing transcript_id"}

            print(f"ðŸ’¾ FinalPersistenceAgent: Updating final record for ID {transcript_id}...")
            try:
                await db_manager.initialize()
                async with db_manager.session_context() as session:
                    # Find the existing record by its ID
                    result = await session.execute(
                        select(Transcript).filter(Transcript.id == transcript_id)
                    )
                    transcript_to_update = result.scalar_one_or_none()

                    if transcript_to_update:
                        # Update the record with all the data from the agent state
                        transcript_to_update.full_text = "\\n".join(state.get("chunks", []))
                        transcript_to_update.extracted_data = state.get("extracted_entities", {})
                        transcript_to_update.crm_data = state.get("crm_data", {})
                        transcript_to_update.social_content = state.get("social_content", {})
                        transcript_to_update.email_draft = state.get("email_draft", "")
                        await session.commit()
                        print(f"   - âœ… Successfully updated transcript ID {transcript_id}.")
                        return {"final_persistence_status": "success"}
                    else:
                        print(f"   - âŒ ERROR: Could not find transcript with ID {transcript_id} to update.")
                        return {"final_persistence_status": "error", "message": "Transcript not found"}
            except Exception as e:
                print(f"   âŒ ERROR in FinalPersistenceAgent: {e}")
                return {"final_persistence_status": "error", "message": str(e)}

    ```

---

### Epic 3.2: Upgrading the `SalesCopilotAgent` as a "Tool"

**Objective:** To transform the `SalesCopilotAgent` from a standalone agent into a powerful, multi-modal "Librarian" tool that the Planner can call. This agent is the primary way our reasoning engine will interact with its long-term memory (Qdrant and Neo4j).

#### **Task 3.2.1: Re-Architect the `SalesCopilotAgent`**

* **Rationale:** In our new architecture, the Sales Copilot is no longer the final step; it is a specialist tool *used during* the reasoning process. It must be upgraded to handle targeted queries, search multiple data sources (vectors and graphs), and return concise information that the Strategist can use.
* **File to Modify:** `agents/sales_copilot/sales_copilot_agent.py`
* **Step 1: Add the Code**
    * The following complete code should replace the existing content of `agents/sales_copilot/sales_copilot_agent.py`. This is a major refactor.

    ```python
    import json
    from typing import List, Dict, Any
    from qdrant_client import models

    from agents.base_agent import BaseAgent
    from config.settings import Settings
    from core.database.qdrant import qdrant_manager
    from core.database.neo4j import neo4j_manager

    class SalesCopilotAgent(BaseAgent):
        """
        Acts as a powerful "Librarian" tool for the reasoning engine. It can
        query the unified knowledge base (Qdrant and Neo4j) to answer specific
        questions posed by the Planner.
        """
        def __init__(self, settings: Settings):
            super().__init__(settings)
            # This agent now needs direct access to the database managers
            self.qdrant_manager = qdrant_manager
            self.neo4j_manager = neo4j_manager

        async def _search_qdrant(self, query: str, doc_type: str = "transcript_chunk", limit: int = 3) -> List[Dict]:
            """Performs a targeted semantic search in Qdrant."""
            print(f"   - SalesCopilot: Searching Qdrant for '{query}' with doc_type '{doc_type}'...")
            search_results = self.qdrant_manager.search(
                query=query,
                limit=limit,
                filter=models.Filter(
                    must=[
                        models.FieldCondition(
                            key="payload.doc_type",
                            match=models.MatchValue(value=doc_type),
                        )
                    ]
                ),
            )
            return [result.payload for result in search_results]

        async def _search_neo4j(self, query: str, params: Dict = None) -> List[Dict]:
            """Executes a read query against the Neo4j knowledge graph."""
            print(f"   - SalesCopilot: Querying Neo4j...")
            return await self.neo4j_manager.execute_read_query(query, params)

        async def run(self, query: str) -> str:
            """
            This is the entry point when the agent is called as a "tool".
            It returns a stringified JSON of its findings.
            """
            print(f"ðŸ¤– SalesCopilotTool activated with query: '{query}'")

            # This is a simple routing logic based on keywords in the query.
            # A more advanced version could use an LLM to determine the best strategy.
            if "objection" in query.lower() and "client" in query.lower():
                # Example of a multi-step query: Graph -> Vector
                print("   - Strategy: Multi-step (Neo4j -> Qdrant)")
                client_name = "Jane Doe" # In a real scenario, this would be extracted from the query
                objections = await self._search_neo4j(
                    "MATCH (c:Client {name: $name})-[:RAISED]->(o:Objection) RETURN o.text as objection",
                    {"name": client_name}
                )
                if objections:
                    # Search Qdrant for context around the specific objection found in the graph
                    objection_text = objections[0]['objection']
                    results = await self._search_qdrant(query=objection_text)
                else:
                    results = [{"error": f"No objections found for client {client_name}"}]
            else:
                # Default to a simple vector search
                print("   - Strategy: Simple Vector Search (Qdrant)")
                results = await self._search_qdrant(query=query)

            # The tool should return a concise, stringified summary for the Strategist
            return json.dumps(results, indent=2)

    ```

---

### Epic 3.3: Finalizing the Master Workflow

**Objective:** To implement the final, production-ready `StateGraph` in the `orchestrator/graph.py` file. This new graph will correctly handle the `transcript_id` dependency, run the intelligence core in parallel, and then seamlessly transition into the reasoning engine for any user-facing queries.

#### **Task 3.3.1: Re-Architect the Orchestrator for the Final Time**

* **Rationale:** This is the culmination of all our design work. We are creating two distinct workflows within one graph:
    1.  **The Ingestion Workflow:** A highly efficient, parallel pipeline for processing new transcripts (`Parser` -> `InitialPersistence` -> `Chunker` -> `Embedder` + `KnowledgeAnalyst` -> `FinalPersistence`).
    2.  **The Reasoning Workflow:** The cognitive loop we designed in Sprint 2 (`Gatekeeper` -> `Planner` -> `ToolExecutor` -> `Auditor` -> `Router` -> `Strategist`).
    
    This final architecture is robust, scalable, and solves all previously identified dependencies.
* **File to Modify:** `orchestrator/graph.py`
* **Step 1: Add the Code**
    * The following complete code should replace the existing content of `orchestrator/graph.py`. This is the final version that integrates all Sprints.

    ```python
    from langgraph.graph import StateGraph, END
    from typing import Dict, Any

    from orchestrator.state import AgentState
    from config.settings import settings

    # --- Import ALL agents for the final architecture ---
    # Ingestion Agents
    from agents.parser.parser_agent import ParserAgent
    from agents.chunker.chunker import ChunkerAgent
    from agents.persistence.initial_persistence_agent import InitialPersistenceAgent
    from agents.embedder.embedder_agent import EmbedderAgent
    from agents.knowledge_analyst.knowledge_analyst_agent import KnowledgeAnalystAgent
    from agents.persistence.final_persistence_agent import FinalPersistenceAgent

    # Reasoning Engine Agents
    from agents.gatekeeper.gatekeeper_agent import GatekeeperAgent
    from agents.planner.planner_agent import PlannerAgent
    from agents.auditor.auditor_agent import AuditorAgent
    from agents.strategist.strategist_agent import StrategistAgent

    # Specialist Tool Agents
    from agents.sales_copilot.sales_copilot_agent import SalesCopilotAgent

    # --- Initialize ALL agents ---
    parser_agent = ParserAgent(settings)
    chunker_agent = ChunkerAgent(settings)
    initial_persistence_agent = InitialPersistenceAgent(settings)
    embedder_agent = EmbedderAgent(settings)
    knowledge_analyst_agent = KnowledgeAnalystAgent(settings)
    final_persistence_agent = FinalPersistenceAgent(settings)
    gatekeeper_agent = GatekeeperAgent(settings)
    planner_agent = PlannerAgent(settings)
    auditor_agent = AuditorAgent(settings)
    strategist_agent = StrategistAgent(settings)
    sales_copilot_agent = SalesCopilotAgent(settings)

    # --- Tool Mapping for the Planner ---
    tool_map = {
        "sales_copilot_tool": sales_copilot_agent,
    }

    # --- Define ALL Nodes for the Final Graph ---

    # --- Ingestion Nodes ---
    async def parser_node(state: AgentState) -> Dict[str, Any]:
        state["raw_text"] = state["file_path"].read_text(encoding='utf-8')
        structured_dialogue = await parser_agent.run(file_path=state["file_path"])
        return {"structured_dialogue": structured_dialogue}

    async def initial_persistence_node(state: AgentState) -> Dict[str, Any]:
        result = await initial_persistence_agent.run(file_path=state["file_path"])
        return {"transcript_id": result["transcript_id"]}

    async def chunker_node(state: AgentState) -> Dict[str, Any]:
        chunks = await chunker_agent.run(file_path=state["file_path"])
        return {"chunks": chunks}

    async def embedder_node(state: AgentState) -> Dict[str, Any]:
        await embedder_agent.run(chunks=state["chunks"], transcript_id=state["transcript_id"])
        return {} # No state change needed, action is external

    async def knowledge_analyst_node(state: AgentState) -> Dict[str, Any]:
        result = await knowledge_analyst_agent.run(chunks=state["chunks"], file_path=state["file_path"])
        return {"extracted_entities": result.get("extracted_entities")}

    async def final_persistence_node(state: AgentState) -> Dict[str, Any]:
        await final_persistence_agent.run(state)
        return {} # Final update, no state change needed

    # --- Reasoning Nodes ---
    async def gatekeeper_node(state: AgentState) -> Dict[str, Any]:
        result = await gatekeeper_agent.run(request=state["original_request"])
        return {"clarification_question": result["clarification_question"]}

    async def planner_node(state: AgentState) -> Dict[str, Any]:
        result = await planner_agent.run(request=state["original_request"])
        return {"plan": result["plan"]}

    async def tool_executor_node(state: AgentState) -> Dict[str, Any]:
        # ... (code from Sprint 2)
        pass

    async def auditor_node(state: AgentState) -> Dict[str, Any]:
        # ... (code from Sprint 2)
        pass

    async def strategist_node(state: AgentState) -> Dict[str, Any]:
        # ... (code from Sprint 2)
        pass
        
    def router_node(state: AgentState) -> str:
        # ... (code from Sprint 2)
        pass

    # --- Master Workflow Construction ---
    def create_master_workflow():
        workflow = StateGraph(AgentState)

        # Add all nodes for both workflows
        workflow.add_node("parser", parser_node)
        workflow.add_node("initial_persistence", initial_persistence_node)
        workflow.add_node("chunker", chunker_node)
        workflow.add_node("embedder", embedder_node)
        workflow.add_node("knowledge_analyst", knowledge_analyst_node)
        workflow.add_node("final_persistence", final_persistence_node)
        # ... (add reasoning nodes here)

        # --- Define the Ingestion Flow ---
        workflow.set_entry_point("parser")
        workflow.add_edge("parser", "initial_persistence")
        workflow.add_edge("initial_persistence", "chunker")
        workflow.add_edge("chunker", "embedder")
        workflow.add_edge("chunker", "knowledge_analyst")
        
        # This join ensures both parallel branches complete before final persistence
        workflow.add_edge(["embedder", "knowledge_analyst"], "final_persistence")
        workflow.add_edge("final_persistence", END)
        
        # The reasoning flow would be a separate entry point or conditional branch
        # For now, we have fully defined the ingestion part.

        return workflow.compile()

    app = create_master_workflow()

    ```
    **Architect's Note:** For clarity, the code for the reasoning nodes (`tool_executor`, `auditor`, etc.) is omitted here but would be pasted in from the Sprint 2 playbook. This final graph correctly establishes the new ingestion pipeline, solving the `transcript_id` dependency by creating the record first, then fanning out to the parallel `embedder` and `knowledge_analyst` nodes, and finally joining them back together for the `final_persistence` update.

---